# -*- coding: utf-8 -*-
"""Project_1_ASHISH_CHANDRA_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCqJ7CZX3NTx6v8Ba_UvNi34LV3nFODX

```
# This is formatted as code
```

#Student Name: ASHISH CHANDRA
#ECE 595 Machine Learning II
#Project 1: CLDNN - Student Code
"""

#Import necessary packages
import keras
import numpy as np
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, Reshape, LSTM
import matplotlib.pyplot as plt
import cPickle

"""#Part 0: Importing and normalizing data"""

#Import dataset and normalize to [0,1]
#Has shape (num_samples, 28, 28)
(data_train, labels_train), (data_test, labels_test) = fashion_mnist.load_data()
data_train = data_train/255.0
data_test = data_test/255.0
data_train = data_train.reshape(60000, 28, 28, 1)
data_test = data_test.reshape(10000, 28, 28, 1)

#Create labels as one-hot vectors
#labels_train and labels_test have shapes (60000, 10) and (10000 10,) respectively
labels_train = keras.utils.np_utils.to_categorical(labels_train, num_classes=10)
labels_test = keras.utils.np_utils.to_categorical(labels_test, num_classes=10)

"""#Part 1: Plotting cross entropy"""

#Show cross-entropy loss function
p_values = np.linspace(0.000001, 0.999999, num=1000)
#print (p_values)
cross_entropy_values = []
for p in p_values:
  cross_entropy_values.append(-1.0*(np.log(p) + np.log(1-p)))
#print (cross_entropy_values)
plt.plot(p_values, cross_entropy_values)
plt.title('-ve log likelyhood VS probability')
plt.ylabel('cross entropy loss')
plt.xlabel('probability - p')

"""Answer the following questions:


1.   How is the the negative log-likelihood of p affected when p is small? How about when p is large?

ANS: **For small values of p i.e. (p close to 0) and for large values of p i.e. (p close to 1), the -ve log likelyhood explodes to infinity. 
 For p ~ 0 and p ~ 1, the cross entropy loss achieves very very large values.**

2.   Why does the behavior of the negative log-likelihood function make it a good objective function for a minimization problem specifically when the output can be interpreted as a probability space?

ANS: **The negative log-likelihood function is a convex function, which makes it a good obective function, as it only has a unique global optima.**

**The expression for the maximum likelihood function is a product of terms and very difficult to differentiate, since log is a monotonic increasing function, so taking log of the maximum likelyhood function does not harm the optimization solution.**

**Taking the log-likelihood function also helps numerically. The product of large number of small probabilities in the likelihood function can easily make the computations loose precision. Once the log is taken the computation changes to summing the log probabilities which is computationally more stable.**

#Part 2: Overfit CNN
"""

#Create and train model architecture
#FILL THIS IN WITH MODEL ARCHITECTURE
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def CNN_overfit():
    #Easiest way to build model in Keras is using Squential. It allows models to 
    #be built layer by layer as we will do here
    model = Sequential ()
    
    # 1st HL : 2D Conv layer with 256 feaure maps & a 3x3 filter
    # first layer requires a specified input shape for 256 features
    model.add(Conv2D(256, 
                   (3,3), 
                   activation = 'relu', 
                   use_bias = 'True',
                   input_shape = (n,n,num_channels)))

    # 2nd HD : 2x2 max pooling layer
    model.add(MaxPooling2D(pool_size = (2,2), 
                           strides = None, 
                           padding = 'valid', 
                           data_format = None))
      
    # 3rd HD : 2D Conv layer with 128 feature maps & a 3x3 filter
    model.add(Conv2D(128, 
                    (3,3), 
                    activation = 'relu', 
                    use_bias = 'True',
                    input_shape = (n,n,num_channels)))
      
    # 4th HD : 2x2 max pooling layer
    model.add(MaxPooling2D(pool_size = (2,2), 
                           strides = None, 
                           padding = 'valid', 
                           data_format = None))
      
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD :  Dense (fully-connected) layer consisting of 100 perceptrons
    model.add(Dense(100,
                    activation = "relu",
                    use_bias = 'True',
                    kernel_initializer = "normal"))
      
    # 7th HD : Dense (fully-connected) layer consisting of 100 perceptrons
    model.add(Dense(100,
                    activation = "relu",
                    use_bias = 'True',
                    kernel_initializer = "normal"))
      
    # Output layer with num_classes = 10 perceptrons
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
CNN_overfit = CNN_overfit()

#Compile model using an appropriate loss and optimizer algorithm
CNN_overfit.compile(loss = keras.losses.CategoricalCrossentropy(),
                    optimizer = 'adam',
                    metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
CNN_overfit_mdata = CNN_overfit.fit(data_train, labels_train,
                                    validation_data = (data_test, labels_test),
                                    epochs = 200,
                                    batch_size = 512,
                                    shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_overfit.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**Note : The "loss" and "accuracy" terms in the output tab generated by the code represent the "loos function value" and "the accuracy of the trained model on the training set". The values represented by "val_loss" and "val_accuracy" represent the "loss function value" and "accuracy of the trained model when run on the testing data". Accuracy of the trained model on testing set : 91.94%**"""

#Plot accuracy vs epoch
plt.plot(CNN_overfit_mdata.history[ 'accuracy' ])
plt.plot(CNN_overfit_mdata.history['val_accuracy'])
plt.title('CNN Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(CNN_overfit_mdata.history['loss'])
plt.plot(CNN_overfit_mdata.history['val_loss'])
plt.title('CNN Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show()

"""Answer the following questions: 



1.   What does the loss of the training set go to? 

ANS: **The training loss goes to zero, to be precise it goes to 3.1946e-08**

2.   What does the loss of the testing set go to? 

ANS: **The loss of the testing set goes down upto 0.25 in the initial 10 - 15 epochs, but then starts to diverge and after 200 epochs it is 1.3555 and seems to keep increasing.**
  
3.   What is the reason for the discrepancy between the training and testing set loss? 

ANS: **For this model, the training loss is 3.1946e-08, where as the testing loss is 1.3555 and seems to keep increasing. Since training loss << testing loss, so this is because of OVERFITTING.**

4.   Explain why the accuracy of the training set, after training, differs so much from the testing set regardless of achieving high training accuracy. Name two ways to avoid this. 

ANS: **Accuracy on the training set is 100%, whereas the accuracy on the testing set is 91.94%. Here, the model does much better on the training set (100% accurate) than on the testing set (91.94% accurate), thus we are OVERFITTING, which reduces generalization and the performance on the testing set.**

**Two ways we can avoid this is :**

**(Regularization) Add constraints on the parameter values (in an explicit way or via norm penalties L1 norm or L2 norm). We can also use early stopping - where we stop the learning process early.**

**Ensemble Methods (Bagging, Boosting) - By having multiple models and enforcing that a majority of these models fit the training set. We can also do early stopping or input data augmentation**

#Part 3: Dropout on input layer
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def CNN_dropout_in():
    model = Sequential()

    # Randomly freezes 30% of the inputs coming into 1st HD (30% dropout - input layer)
    model.add(Dropout(0.30))

    # 1st HD : Build a 2D conv layer with 256 feature mapsa and a 3x3 filter
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n, num_channels)))


    # 2nd HD : a 2x2 max pooling 
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D conv layer with 128 feature maps and a 3x3 filter
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n, num_channels)))
    
    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))
    return model

#Create instance of CNN model graph
CNN_dropout_in = CNN_dropout_in()

#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_in.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
CNN_dropout_in_mdata = CNN_dropout_in.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_dropout_in.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**Accuracy of the trained model on the testing set after 200 epochs is 87.76%**"""

#Plot accuracy vs epoch
plt.plot(CNN_dropout_in_mdata.history['accuracy'])
plt.plot(CNN_dropout_in_mdata.history['val_accuracy'])
plt.title('CNN dropout in Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(CNN_dropout_in_mdata.history['loss'])
plt.plot(CNN_dropout_in_mdata.history['val_loss'])
plt.title('CNN dropout Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show()

"""Answer the following questions: 



1.   What does the loss of the training set go to? 

ANS: **The loss of the training set after 200 epochs goes to 0.0339**
  
2.   What does the loss of the testing set go to? 

ANS: **The loss of the testing set after 200 epochs goes to 0.5398**

3.   Why is the loss of the training set so different from the loss of the testing set regardless of using dropout? 

ANS: **Here we considered dropouts only on the input layer. The training set loss is: 0.0339 & training set accuracy is: 0.9875, while the testing set loss is: 0.5398 and testing set accuracy is: 0.8776.**

**The dropout applied to the input layer, drops certain input features chosen at random on each iteration of the algorithm. The dropout did not help in this case, because we could be needing all the input features for a better performance, but droping some features everytime could be eliminating important features which are required for better learning.**

#Part 4-I: Dropout on Hidden Layers
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def CNN_dropout_hidden():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
CNN_dropout_hidden = CNN_dropout_hidden()


#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_hidden.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
CNN_dropout_hidden_mdata = CNN_dropout_hidden.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_dropout_hidden.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**Accuracy of the trained model on the testing set after 200 epochs is 91.87%**"""

#Plot accuracy vs epoch
plt.plot(CNN_dropout_hidden_mdata.history[ 'accuracy' ])
plt.plot(CNN_dropout_hidden_mdata.history['val_accuracy'])
plt.title('CNN dropout hidden Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(CNN_dropout_hidden_mdata.history['loss'])
plt.plot(CNN_dropout_hidden_mdata.history['val_loss'])
plt.title('CNN dropout hidden Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show()

"""Answer the following questions:



1.   What does the loss of the training set go to? 

ANS: **The loss of the training set goes to 0.0167**
  
2.   What does the loss of the testing set go to? 

ANS: **The loss of the testing set goes to 0.4101**

3.   Why was using dropout more effective here in comparison to the architecture in Part 3? 

ANS: **Using dropouts at the 2 convolutional hidden layers was more efficient than using dropout only at the input layer.**

**When dropout was used at the two 2D convolutional layers: training accuracy - 99.48% & training loss - 0.0167, while testing accuracy - 91.87% & testing loss - 0.4101.**

**In Part 3, when we only used dropout at the input layer: training accuracy -  98.75%  & training loss - 0.0339, while testing accuracy 87.76% & testing loss - 0.5398.**

**Droputs at the two 2D conv layers is more effective than dropout at input layer because:**

**1) Usually it is more effective to place dropout on layers with large number of connections (because they are the ones with larger number of parameters). Since the 2D Conv layers have the largest number of parameters associated to them, they are more likely to be responsible for overfitting.**

**2) Dropout at input layers removes features, and it could be possible that for a good performance we need to have all the input features.**
  
4.   What is the difference in accuracy, after training, between the training and testing set? Is the model a good fit for the data? 

ANS: **After training the accuracy on the training set is 99.48% and the accuracy of the testing set is 91.87%, the difference in the accuracy is 7.61%. Usually, we prefer the accuracy difference to be less than equal to 1% for declaring a model to be a good fit. So, THIS IS NOT A GOOD FIT.**

#Part 4-II [Sub-Question 5]: Dropout on Input and Hidden Layers
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def CNN_dropout_both():

    #Easiest way to build model in Keras is using Squential. It allows model to be build layer by layer as we will do here
    model = Sequential()

    # 30% dropout at the input layer
    model.add(Dropout(0.30))

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))
    
    return model


#Create instance of CNN model graph
CNN_dropout_both = CNN_dropout_both()

#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_both.compile(loss = keras.losses.CategoricalCrossentropy(),
                         optimizer = 'adam',
                         metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
CNN_dropout_both_mdata = CNN_dropout_both.fit(data_train, labels_train,
                                              validation_data = (data_test, labels_test),
                                              epochs = 200,
                                              batch_size = 512,
                                              shuffle = True)

# Evaluate the accuracy of the model on the testing set
scores = CNN_dropout_both.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy : %.2f%%" % (scores[1]*100))

#Plot accuracy vs epoch
plt.plot(CNN_dropout_both_mdata.history[ 'accuracy' ])
plt.plot(CNN_dropout_both_mdata.history['val_accuracy'])
plt.title('CNN dropout both Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(CNN_dropout_both_mdata.history['loss'])
plt.plot(CNN_dropout_both_mdata.history['val_loss'])
plt.title('CNN dropout both Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show()

"""Answer the following questions: 

1.   What does the loss of the training set go to? 

ANS: **The loss of the training set after 200 epochs goes to 0.0731**
  
2.   What does the loss of the testing set go to? 

ANS: **The loss of the testing set after 200 epochs goes to 0.4776**
  
3.   What is the difference in accuracy, after training, between the training and testing set?  

ANS: **After training the accuracy on the training set is 97.18%, while the accuracy on the testing set is 84.99%. The difference between the two accuracies is 12.19%**

4.   Compare results in 4 with results in 5. Comment on robustness and accuracy.

ANS: **In (4), using dropouts at the two 2Dconv layers, the training loss: 0.0167 & training accuracy: 99.48%, while the testing loss: 0.4101 & testing accuracy: 91.87%. The difference in the accuracy over the training and testing set here is 7.61%.**
  
  
**In (5), using dropout at the input layer and the two 2D conv layers, the training loss: 0.0731 & training accuracy: 97.18% , while the testing loss: 0.4776 & testing accuracy: 84.99%. The difference in the accuracy over the training and testing set here is 12.19%.**

**Since the accuracy of (4) both on training and testing data is a more than the accuracy of (5), so in terms of accuracy (4) is better, but still (4) and (5) are overfitting.**

**(5) is more robust to inputs than (4), since it takes care of the randomness over the input layer, which is introduced by adding dropouts at the input layer.**

#Part 5 : Activation Functions
"""

#Create and train model architecture
def CNN_dropout_both_act():

    #Easiest way to build model in Keras is using Squential. It allows model to be build layer by layer as we will do here
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron USING SIGMOID
    model.add(Dense(num_classes,
                    activation = "sigmoid"))

    return model


#Create instance of CNN model graph
CNN_dropout_both_act = CNN_dropout_both_act()

#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_both_act.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])


#Train the model and assign training meta-data to a variable
CNN_dropout_both_act_mdata = CNN_dropout_both_act.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_dropout_both_act.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

#Plot accuracy vs epoch
plt.plot(CNN_dropout_both_act_mdata.history[ 'accuracy' ])
plt.plot(CNN_dropout_both_act_mdata.history['val_accuracy'])
plt.title('CNN dropout both act SIGMOID - output - Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show( )

#Plot loss vs epoch
plt.plot(CNN_dropout_both_act_mdata.history['loss'])
plt.plot(CNN_dropout_both_act_mdata.history['val_loss'])
plt.title('CNN dropout both act SIGMOID - Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""Part 5-1 : Use the ‘sigmoid’ activation function for the output layer and compare the results with the softmax results.

**For Part 5 : We keep its architecture same as Part 4, 4th question and use Sigmoid activation funciton at the output layer, the training loss is: 0.0170 & training accuracy is: 99.42%, while the testing loss is: 0.4006 & training accuracy is: 91.70%.**

**From Part 4, point 5, we obtain the following results: Training loss is: 0.0731 & training accuracy is: 97.18%, while testing loss is: 0.4776 & testing accuracy is: 84.99%**

**From the reported results it seems that the model in Part 5 performs better than the model in Part 4, point 5.**

Using TANH as the output activation function - Loss Function is Cross Entropy Function
"""

#Create and train model architecture
def CNN_dropout_both_act():

    #Easiest way to build model in Keras is using Squential. It allows model to be build layer by layer as we will do here
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron USING TANH
    model.add(Dense(num_classes,
                    activation = "tanh"))

    return model


#Create instance of CNN model graph
CNN_dropout_both_act = CNN_dropout_both_act()

#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_both_act.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])


#Train the model and assign training meta-data to a variable
CNN_dropout_both_act_mdata = CNN_dropout_both_act.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_dropout_both_act.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

#Plot accuracy vs epoch
plt.plot(CNN_dropout_both_act_mdata.history[ 'accuracy' ])
plt.plot(CNN_dropout_both_act_mdata.history['val_accuracy'])
plt.title('CNN dropout both act TANH - output - Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show( )

#Plot loss vs epoch
plt.plot(CNN_dropout_both_act_mdata.history['loss'])
plt.plot(CNN_dropout_both_act_mdata.history['val_loss'])
plt.title('CNN dropout both act TANH - Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""**''tanh'' as output activation DOES NOT WORK with cross entropy loss.** The plots obtained above clearly show that tanh output activation with cross entropy is useless. 

**REASON: The range of tanh is -1 to 1, which is not allowed while using cross entropy function. The cross entropy requires entries from the interval 0 to 1 and all entries summing upto 1 (requires them to be probabilities), which is not the case when using tanh. Also cross entropy computes logarithms, which is not defined for negative values.**

**''sigmoid'' does not cause the same issues as tanh when used
with Cross entropy loss.**

**''Sigmoid'' is different from tanh in the sense that its range is in 0 to 1. Due to this fact it is useful in prediction of probabilities. Moreover, when used with cross entropy, log is well defined for the range of sigmoid function i.e interval 0 to 1.**

Using TANH as the output activation function - Loss Function is MEAN SQUARED ERROR
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10
def CNN_dropout_both_act():

    #Easiest way to build model in Keras is using Squential. It allows model to be build layer by layer as we will do here
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron USING TANH
    model.add(Dense(num_classes,
                    activation = "tanh"))

    return model


#Create instance of CNN model graph
CNN_dropout_both_act = CNN_dropout_both_act()

#Compile model using an appropriate loss and optimizer algorithm
CNN_dropout_both_act.compile(loss = keras.losses.MeanSquaredError(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])


#Train the model and assign training meta-data to a variable
CNN_dropout_both_act_mdata = CNN_dropout_both_act.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = CNN_dropout_both_act.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**The training loss is: 0.0015 & training accuracy is: 99.18%. The testing loss is: 0.0133 & testing accuracy is: 91.89%.**"""

#Plot accuracy vs epoch
plt.plot(CNN_dropout_both_act_mdata.history[ 'accuracy' ])
plt.plot(CNN_dropout_both_act_mdata.history['val_accuracy'])
plt.title('CNN dropout both act TANH & MSE- output - Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(CNN_dropout_both_act_mdata.history['loss'])
plt.plot(CNN_dropout_both_act_mdata.history['val_loss'])
plt.title('CNN dropout both act TANH & MSE - Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""Answer the following questions: 

1-c.   Compare the above results with softmax results (with point 5 results of Part 4-II)

ANS: **For Part 5 : We keep its architecture same as Part 4, 4th question and use Sigmoid activation funciton at the output layer, the training loss is: 0.0170 & training accuracy is: 99.42%, while the testing loss is: 0.4006 & training accuracy is: 91.70%.**

**From Part 4, point 5, we obtain the following results: Training loss is: 0.0731 & training accuracy is: 97.18%, while testing loss is: 0.4776 & testing accuracy is: 84.99%**

**From the reported results it seems that the model in Part 5 performs better than the model in Part 4, point 5.**
  
2-a.   Does ‘tanh’ as output activation work with Cross-entropy loss?

ANS:**''tanh'' as output activation DOES NOT WORK with cross entropy loss. The plots obtained above clearly show that tanh output activation with cross entropy is useless.**

2-b.   Give a reason why tanh is not recommended with cross entropy loss?

ANS: **REASON: The range of tanh is -1 to 1, which is not allowed while using cross entropy function. The cross entropy requires entries from the interval 0 to 1 and all entries summing upto 1 (requires them to be probabilities), which is not the case when using tanh. Also cross entropy computes logarithms, which is not defined for negative values.**

2-c.   Give a reason why sigmoid does not cause problem as tanh with Cross entropy loss?

ANS: **''sigmoid'' does not cause the same issues as tanh when used
with Cross entropy loss.**

**''Sigmoid'' is different from tanh in the sense that its range is in 0 to 1. Due to this fact it is useful in prediction of probabilities. Moreover, when used with cross entropy, log is well defined for the range of sigmoid function i.e interval 0 to 1.**

BONUS QUESTION

Changing 1st Conv2D Hidden layer activations to Sigmoid & second Conv2D hidden layer to Tanh - Architecture same as Part 4, Q4 with 1st and 2nd hidden activations changed to Sigmoid & Tanh respectively.
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_a():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'sigmoid',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'tanh',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_a = BONUS_a()


#Compile model using an appropriate loss and optimizer algorithm
BONUS_a.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_a_mdata = BONUS_a.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = BONUS_a.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

#Plot accuracy vs epoch
plt.plot(BONUS_a_mdata.history[ 'accuracy' ])
plt.plot(BONUS_a_mdata.history['val_accuracy'])
plt.title('Bonus_a Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(BONUS_a_mdata.history['loss'])
plt.plot(BONUS_a_mdata.history['val_loss'])
plt.title('BONUS_a Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""BONUS_a : ALL hidden layers are SIGMOID - Architecture same as Part 4, Q4 with ALL hidden activations changed to Sigmoid """

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_a():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'sigmoid',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'sigmoid',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'sigmoid',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'sigmoid',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_a = BONUS_a()


#Compile model using an appropriate loss and optimizer algorithm
BONUS_a.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_a_mdata = BONUS_a.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = BONUS_a.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

#Plot accuracy vs epoch
plt.plot(BONUS_a_mdata.history[ 'accuracy' ])
plt.plot(BONUS_a_mdata.history['val_accuracy'])
plt.title('Bonus_a Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(BONUS_a_mdata.history['loss'])
plt.plot(BONUS_a_mdata.history['val_loss'])
plt.title('BONUS_a Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""BONUS_a : ALL hidden layers are TANH - Architecture same as Part 4, Q4 with ALL hidden activations changed to TANH """

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_a():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'tanh',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'tanh',
                     use_bias = 'True',
                     input_shape = (n,n,num_channels)))
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'tanh',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'tanh',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_a = BONUS_a()


#Compile model using an appropriate loss and optimizer algorithm
BONUS_a.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_a_mdata = BONUS_a.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 200,
                                       batch_size = 512,
                                       shuffle = True)

# Evaluateing accuracy of model on testing set
scores = BONUS_a.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

#Plot accuracy vs epoch
plt.plot(BONUS_a_mdata.history[ 'accuracy' ])
plt.plot(BONUS_a_mdata.history['val_accuracy'])
plt.title('Bonus_a Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(BONUS_a_mdata.history['loss'])
plt.plot(BONUS_a_mdata.history['val_loss'])
plt.title('BONUS_a Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""BONUS_b : ReLu at Hidden layer activation, comparing average activity

https://jhui.github.io/2018/02/11/Keras-tutorial/

https://faroit.com/keras-docs/1.0.6/getting-started/faq/#how-can-i-visualize-the-output-of-an-intermediate-layer

https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction

https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0
"""

#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_b():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '0th_layer_first_relu',
                     input_shape = (n,n,num_channels)))
        
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '3rd_layer_second_relu',
                     input_shape = (n,n, num_channels)))
                     # having or not having the input_shape here does not a difference
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_b = BONUS_b()




#Compile model using an appropriate loss and optimizer algorithm
BONUS_b.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_b_mdata = BONUS_b.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 20,
                                       batch_size = 1024,
                                       shuffle = True)

BONUS_b.summary()







# ONE METHOD TO DO THE REQUIRED COUNTING
'''
from keras.models import Model
layer_outputs = [layer.output for layer in BONUS_b.layers[:8]] 
activation_model = Model(inputs=BONUS_b.input, outputs=layer_outputs)
activations = activation_model.predict(data_train[0:1]) 
first_layer_activation = activations[0]
print(first_layer_activation.shape)
print(np.where(first_layer_activation != 0,1,0).sum())
plt.matshow(first_layer_activation[0,:,:,1], cmap = 'viridis')
plt.show()
'''




# BONUS QUESTION PART b) (i) Calculating number of active neurons on fixed images
# Fixing images indexed by 0, 20000, 40000, 59999 in the training set
from keras.models import Model
images = [0, 20000, 40000, 59999]
layers_with_relu = [0,3,7,8]
for i in images:
  print (" ")
  print ("For image indexed :",i,"in the training set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()

# Fixing images indexed by 0, 3000, 6000, 9999 in the training set
test_images = [0, 3000, 6000, 9999]
for i in test_images:
  print (" ")
  print ("For image indexed :",i,"in the testing set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()



# Evaluateing accuracy of model on testing set
scores = BONUS_b.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**After 20 epochs the activity result is obtained as :**

('For image indexed :', 0, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 53.019831730769226, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 42.21978305785124, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 53.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 53.0, '%')
 
--------------------------------------------------
('For image indexed :', 20000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 50.16468657544378, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 38.98502066115703, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 55.00000000000001, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 59.0, '%')
 
--------------------------------------------------
('For image indexed :', 40000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 49.95088295118343, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 38.604080578512395, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 56.99999999999999, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 57.99999999999999, '%')

 --------------------------------------------------
('For image indexed :', 59999, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 49.80064256656805, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 38.15211776859504, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 64.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 59.0, '%')
 
--------------------------------------------------
('For image indexed :', 0, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 53.019831730769226, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 42.21978305785124, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 53.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 53.0, '%')
 
--------------------------------------------------
('For image indexed :', 3000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 53.04930196005917, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 41.01885330578512, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 54.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 51.0, '%')

 
--------------------------------------------------
('For image indexed :', 6000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 52.725129437869825, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 41.23837809917356, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 56.00000000000001, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 57.99999999999999, '%')

 --------------------------------------------------
('For image indexed :', 9999, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 54.76550943047337, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 43.80810950413223, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 54.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 57.99999999999999, '%')
"""

# Doing it for 30 epochs
#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_b():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '0th_layer_first_relu',
                     input_shape = (n,n,num_channels)))
        
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '3rd_layer_second_relu',
                     input_shape = (n,n, num_channels)))
                     # having or not having the input_shape here does not a difference
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_b = BONUS_b()




#Compile model using an appropriate loss and optimizer algorithm
BONUS_b.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_b_mdata = BONUS_b.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 30,
                                       batch_size = 1024,
                                       shuffle = True)

BONUS_b.summary()







#  TO DO THE REQUIRED COUNTING
'''
from keras.models import Model
layer_outputs = [layer.output for layer in BONUS_b.layers[:8]] 
activation_model = Model(inputs=BONUS_b.input, outputs=layer_outputs)
activations = activation_model.predict(data_train[0:1]) 
first_layer_activation = activations[0]
print(first_layer_activation.shape)
print(np.where(first_layer_activation != 0,1,0).sum())
plt.matshow(first_layer_activation[0,:,:,1], cmap = 'viridis')
plt.show()
'''




# BONUS QUESTION PART b) (i) Calculating number of active neurons on fixed images
# Fixing images indexed by 0, 20000, 40000, 59999 in the training set
from keras.models import Model
images = [0, 20000, 40000, 59999]
layers_with_relu = [0,3,7,8]
for i in images:
  print (" ")
  print ("For image indexed :",i,"in the training set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()

# Fixing images indexed by 0, 3000, 6000, 9999 in the training set
test_images = [0, 3000, 6000, 9999]
for i in test_images:
  print (" ")
  print ("For image indexed :",i,"in the testing set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()



# Evaluateing accuracy of model on testing set
scores = BONUS_b.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**After 30 epochs the activity result is obtained as:**

('For image indexed :', 0, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 49.538877588757394, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 34.826962809917354, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 50.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 61.0, '%')

------------------------------------------------------
 
('For image indexed :', 20000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 44.229035687869825, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 30.126549586776857, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 51.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 59.0, '%')

---------------------------------------------------------
 
('For image indexed :', 40000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 43.906018860946745, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 30.087809917355372, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 52.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 63.0, '%')

--------------------------------------------------------
 
('For image indexed :', 59999, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 43.722841161242606, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 29.325929752066116, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 53.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 60.0, '%')

------------------------------------------------------
 
('For image indexed :', 0, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 49.538877588757394, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 34.826962809917354, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 50.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 61.0, '%')

------------------------------------------------------
 
('For image indexed :', 3000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 51.19036612426036, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 34.01988636363637, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 44.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 47.0, '%')

----------------------------------------------------
 
('For image indexed :', 6000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 51.49431397928994, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 33.955320247933884, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 54.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 73.0, '%')

-----------------------------------------------------
 
('For image indexed :', 9999, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 53.16082655325444, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 37.06740702479338, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 48.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 48.0, '%')
"""

# Doing it for 80 epochs
#Create and train model architecture
num_samples = 60000
n = 28
num_channels = 1
num_classes = 10

def BONUS_b():
    model = Sequential()

    # 1st HD : 2D Conv layer with 256 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(256,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '0th_layer_first_relu',
                     input_shape = (n,n,num_channels)))
        
    # 1st HD : 30 % dropout of 1st Hidden layer
    model.add(Dropout(0.30))

    # 2nd HD : a 2x2 max-pooling layer
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 3rd HD : 2D Conv layer with 128 feature maps and a 3x3 filter with 30% dropout
    model.add(Conv2D(128,
                     (3,3),
                     activation = 'relu',
                     use_bias = 'True',
                     name = '3rd_layer_second_relu',
                     input_shape = (n,n, num_channels)))
                     # having or not having the input_shape here does not a difference
    
    # 3rd HD : 30% Dropout of the 3rd Hidden layer
    model.add(Dropout(0.30))

    # 4th HD : 2x2 max pooling
    model.add(MaxPooling2D(pool_size = (2,2),
                           strides = None,
                           padding = 'valid',
                           data_format = None))
    
    # 5th HD : Layer to flatten the data
    model.add(Flatten())

    # 6th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # 7th HD : A dense layer with 100 perceptron
    model.add(Dense(100,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = "normal"))
    
    # output layer with num_class = 10 perceptron 
    model.add(Dense(num_classes,
                    activation = "softmax"))

    return model

#Create instance of CNN model graph
BONUS_b = BONUS_b()




#Compile model using an appropriate loss and optimizer algorithm
BONUS_b.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
BONUS_b_mdata = BONUS_b.fit(data_train, labels_train,
                                       validation_data = (data_test, labels_test),
                                       epochs = 80,
                                       batch_size = 1024,
                                       shuffle = True)

BONUS_b.summary()







# ONE METHOD TO DO THE REQUIRED COUNTING
'''
from keras.models import Model
layer_outputs = [layer.output for layer in BONUS_b.layers[:8]] 
activation_model = Model(inputs=BONUS_b.input, outputs=layer_outputs)
activations = activation_model.predict(data_train[0:1]) 
first_layer_activation = activations[0]
print(first_layer_activation.shape)
print(np.where(first_layer_activation != 0,1,0).sum())
plt.matshow(first_layer_activation[0,:,:,1], cmap = 'viridis')
plt.show()
'''




# BONUS QUESTION PART b) (i) Calculating number of active neurons on fixed images
# Fixing images indexed by 0, 20000, 40000, 59999 in the training set
from keras.models import Model
images = [0, 20000, 40000, 59999]
layers_with_relu = [0,3,7,8]
for i in images:
  print (" ")
  print ("For image indexed :",i,"in the training set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()

# Fixing images indexed by 0, 3000, 6000, 9999 in the training set
test_images = [0, 3000, 6000, 9999]
for i in test_images:
  print (" ")
  print ("For image indexed :",i,"in the testing set")
  for l in layers_with_relu:
    XX = []
    YY = []
    XX = BONUS_b.input
    YY = BONUS_b.layers[l].output
    new_model = Model(XX,YY)
    Xaug = data_train[i:i+1]
    Xresult = new_model.predict(Xaug)
    if (l==0):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first 2D Conv layer is :", (float(count)/173056)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==3):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second 2D Conv layer is :", (float(count)/15488)*100,'%')
      #plt.matshow(Xresult[0,:,:,1], cmap = 'viridis')
      #plt.show()
    elif (l==7):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at first dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()
    elif (l==8):
      count = np.where(Xresult != 0,1,0).sum()
      print ("# of non zero Relu outputs at second dense 100 perceptron layer is:", (float(count)/100)*100,'%')
      #plt.matshow(Xresult[:,:], cmap = 'viridis')
      #plt.show()



# Evaluateing accuracy of model on testing set
scores = BONUS_b.evaluate(data_test, labels_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**After 80 epochs the activity is obtained as:**

('For image indexed :', 0, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 38.09056028106509, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 22.94034090909091, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 42.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 56.99999999999999, '%')

----------------------------------------------------
 
('For image indexed :', 20000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 31.690319896449704, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 20.816115702479337, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 33.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 54.0, '%')

----------------------------------------------------------
 
('For image indexed :', 40000, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 30.666951738165682, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 19.860537190082646, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 33.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 53.0, '%')

-------------------------------------------------------------
 
('For image indexed :', 59999, 'in the training set')

('# of non zero Relu outputs at first 2D Conv layer is :', 31.157544378698226, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 18.40779958677686, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 44.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 57.99999999999999, '%')

---------------------------------------------------------------
 
('For image indexed :', 0, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 38.09056028106509, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 22.94034090909091, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 42.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 56.99999999999999, '%')

-------------------------------------------------------------
 
('For image indexed :', 3000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 36.974736501479285, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 20.577221074380166, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 43.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 56.00000000000001, '%')

------------------------------------------------------------
 
('For image indexed :', 6000, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 42.67578125, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 18.259297520661157, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 45.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 65.0, '%')

--------------------
 
('For image indexed :', 9999, 'in the testing set')

('# of non zero Relu outputs at first 2D Conv layer is :', 38.83136094674556, '%')

('# of non zero Relu outputs at second 2D Conv layer is :', 24.857954545454543, '%')

('# of non zero Relu outputs at first dense 100 perceptron layer is:', 43.0, '%')

('# of non zero Relu outputs at second dense 100 perceptron layer is:', 63.0, '%')

#Part 6: Creating a CLDNN

Run the code in the block below 'as is.' After executing, the high SNR RadioML training and testing data will be stored in the arrays X_train and X_test, respectively. Their respective one-hot labels will be stored in Y_train and Y_test.
"""

#Download RML 2016.10b dataset and untar file
!wget http://opendata.deepsig.io/datasets/2016.10/RML2016.10b.tar.bz2
!tar -xvjf RML2016.10b.tar.bz2

#Extract high SNR data and obtain their corresponding lables 
Xd = cPickle.load(open("RML2016.10b.dat",'rb'))
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
X = []
lbl = []
for mod in mods:
    for snr in snrs:
        if snr > 0:
            X.append(Xd[(mod,snr)])
            for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)

np.random.seed(2016)
n_examples = X.shape[0]
n_train = n_examples * 0.8
n_train = int(n_train)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]
def to_onehot(yy):
    yy1 = np.zeros([len(yy), max(yy)+1])
    yy1[np.arange(len(yy)),yy] = 1
    return yy1
Y_train = to_onehot(map(lambda x: mods.index(lbl[x][0]), train_idx))
Y_test = to_onehot(map(lambda x: mods.index(lbl[x][0]), test_idx))

#Re-shape data to appropriate dimensions 
X_train = X_train.reshape(432000, 2, 128, 1)
X_test = X_test.reshape(108000, 2, 128, 1)

#Create and train model architecture
num_samples = 60000
n1 = 2
n2 = 128
num_channels = 1
num_classes = 10

def cldnn():
    model = Sequential()
    
    # HD 1
    model.add(Conv2D(256,
                     (1,3),                     
                     use_bias = 'True',
                     activation = 'relu',
                     input_shape = (n1,n2,num_channels)))
    
    model.add(Dropout(0.20))

    # HD 2
    model.add(Conv2D(256,
                     (2,3),
                     use_bias = 'True',
                     activation = 'relu'))
  
    # HD 3
    model.add(Conv2D(80,
                     (1,3),
                     use_bias = 'True',
                     activation = 'relu'))
    
    model.add(Dropout(0.20))

    # HD 4
    model.add(Conv2D(80,
                     (1,3),
                     use_bias = 'True',
                     activation = 'relu'))

    model.add(Reshape((200, 48)))

    # HD 5
    model.add(LSTM(50, return_sequences = False))

    # HD 6
    model.add(Dense(128,
                    activation = 'relu',
                    use_bias = 'True',
                    kernel_initializer = 'normal'))
    
    # output layer 
    model.add(Dense(num_classes,
                    use_bias = 'True',
                    activation = 'softmax'))
    return model

#Create instance of CNN model graph
cldnn = cldnn()
cldnn.summary()

#Compile model using an appropriate loss and optimizer algorithm
cldnn.compile(loss = keras.losses.CategoricalCrossentropy(),
                       optimizer = 'adam',
                       metrics = ['accuracy'])

#Train the model and assign training meta-data to a variable
cldnn_mdata = cldnn.fit(X_train, Y_train,
                                       validation_data = (X_test, Y_test),
                                       epochs = 80,
                                       batch_size = 1024,
                                       shuffle = True)


# Evaluateing accuracy of model on testing set
scores = cldnn.evaluate(X_test, Y_test)

#Print accuracy of model on testing set after training 
print ("Accuracy: %.2f%%" %(scores[1]*100))

"""**The trained model has training loss of: 0.1308 & and training accuracy of: 92.98%. For the testing set the testing loss is: 0.1364 & the testing accuracy is: 92.81%.**

**Note: The Reason I ran it for 80 epochs is since in one of my previous runs of 100 epochs, whose results i lost (since I had to rerun the code), I found that the losses and accuracies do not change by a significant amout from what I have repoted here with 80 epochs. The improvements in the results were insignificant as compared to 100 epochs, but the time taken was large because the code was very slow.**

"""

#Plot accuracy vs epoch
plt.plot(cldnn_mdata.history['accuracy'])
plt.plot(cldnn_mdata.history['val_accuracy'])
plt.title('cldnn Accuracy vs . Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train','test'] , loc= 'upper left')
plt.show()

#Plot loss vs epoch
plt.plot(cldnn_mdata.history['loss'])
plt.plot(cldnn_mdata.history['val_loss'])
plt.title('cldnn Loss vs . Epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'] , loc= 'upper left')
plt.show( )

"""Answer the following questions: 



1.   Is this model a good fit for the data?  

ANS: **The trained model has training loss of: 0.1308 & and training accuracy of: 92.98%. For the testing set the testing loss is: 0.1364 & the testing accuracy is: 92.81%.**

**The difference in the accuracy of the trained model over training and testing data is 0.17%, with training and testing losses also low. YES, IT IS A GOOD FIT FOR THE DATA.**

2.   Give two ways to improve the robustness of the model.

ANS: **The robustness of the model can be improved by doing NOISE INJECTION during training. The noise can be added to weights, hidden layers, input vector or output layer.**
"""