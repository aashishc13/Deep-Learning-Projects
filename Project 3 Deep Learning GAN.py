# -*- coding: utf-8 -*-
"""Project_3_ASHISH_CHANDRA_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BnXMcgEFamzU_WfNQueo1_8HZpByXUIB

#Student Name: ASHISH CHANDRA
#ECE 595 Machine Learning II
#Project 3: GAN - Student Code
"""

#Import necessary packages
import numpy as np
import keras
from keras.layers import Dense, Dropout, Input
from keras.models import Model,Sequential
from keras.datasets import mnist
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import adam
from keras.models import load_model
import matplotlib.pyplot as plt

"""#Part 1: Implementing the GAN"""

#Load MNIST data and normalize to [-1, 1]

#Load MNIST data and normalize to [0,1]
(data_train, _), (data_test, _) = mnist.load_data()
data_train = data_train/255.0
data_test = data_test/255.0
# Shifting the above data in [-0.5, 0.5] by substracting 0.5 from above
data_train = data_train - 0.5
data_test = data_test - 0.5
#  Now multiplying the above by 2 to scale it to [-1,1]
data_train = data_train * 2
data_test = data_test * 2
print("shape training", np.shape(data_train))
print("shape testing", np.shape(data_test))

"""**WITH DROPOUTS OF 0.30**"""

# The D-dimensional noise vector length
latent_dim = 100

# Optimizer for discriminator, which will have a higher learning rate than adversarial model
def adam_optimizer():
    return adam(lr = 0.2, beta_l = 0.5)

def dis_optimizer():
    #return adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5)

def gan_optimizer():
    #return adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5) 




# Genrerator model
dim = 7
depth = 64+64+64+64 #256
def create_generator():
    generator = Sequential()
    generator.add(Dense(depth, input_dim = latent_dim))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(512))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(1024))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(28*28, activation = 'tanh'))

    #generator.compile(loss = keras.losses.BinaryCrossentropy(),
    #                      optimizer = gan_optimizer(),
    #                      metrics = ['accuracy'])
    return generator

# Discriminator model
def create_discriminator():
    discriminator = Sequential()
    discriminator.add(Dense(1024, input_dim = 28*28))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    discriminator.add(Dropout(0.30)) #0.25

    discriminator.add(Dense(512))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    discriminator.add(Dropout(0.30)) #0.25

    discriminator.add(Dense(256))
    discriminator.add(LeakyReLU(alpha = 0.2))

    discriminator.add(Dense(units = 1, activation = 'sigmoid'))
    discriminator.compile(loss = keras.losses.BinaryCrossentropy(),
                          optimizer = dis_optimizer(),
                          metrics = ['accuracy'])
    return discriminator

# Create adversarial model
def create_gan(discriminator, generator):
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(inputs = gan_input, outputs = gan_output)
    gan.compile(loss = keras.losses.BinaryCrossentropy(),
                optimizer = gan_optimizer(),
                metrics = ['accuracy'])
    return gan

# Creating graph for GAN
generator = create_generator()
discriminator = create_discriminator()
gan = create_gan(discriminator, generator)



# ALGORITHM 1
# Model and training parameters
#ASSIGN VALUES TO THE FOLLOWING VARIABLES
epochs = 100000          # debug using small epochs 5
batch_size = 1024 
sample_interval = 10000      # debug using small interval 1

plot_loss_descriminator = []
plot_acc_descriminator = []
plot_loss_gan = []
plot_acc_gan = []

# Array to save training history
training_meta_data = np.zeros([epochs, 4])

# Training the GAN
for e in range(1, epochs+1):

    # Generate random noise as input
    noise = np.random.normal(0,1, size = (batch_size, latent_dim)) #-0.10

    # Generate fake MNIST images from generated noise
    fake_image = generator.predict(noise)
    #print("fake image shape :", fake_image.shape)

    # Get a random set of real MNIST images
    real_image = data_train[np.random.randint(0, data_train.shape[0], size = batch_size)]
    real_image = real_image.reshape(batch_size, 28*28)
    #print(" Real image shape :", real_image.shape)

    # Concatenate real and fake images into a single array (or batch)
    data_total = np.concatenate([real_image, fake_image])

    # Assign training labels (assign high probability, but not 1, to real images)
    labels_real = np.ones((batch_size))*0.9
    labels_fake = np.zeros((batch_size)) #0.10 
    labels_dis = np.concatenate([labels_real, labels_fake])

    # Allow discriminator parameters to be updated
    discriminator.trainable = True

    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable
    d_loss = discriminator.train_on_batch(data_total, labels_dis)
    plot_loss_descriminator.append(d_loss[0])
    plot_acc_descriminator.append(d_loss[1])


    # Train adversarial model and try to fool discriminator (with incorrect label) 
    # by generating a new batch of noise and assign them labels of real data
    noise = np.random.normal(0,1, size = (batch_size, latent_dim))#-0.10
    labels_gen = np.ones((batch_size))   #*0.9

    # Keep discriminator weights constant while training generator
    discriminator.trainable = False

    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable
    gan_loss = gan.train_on_batch(noise, labels_gen)
    plot_loss_gan.append(gan_loss[0])
    plot_acc_gan.append(gan_loss[1])


    # Save training status
    # Discriminator and model loss
    training_meta_data[e-1, 0] = d_loss[0]
    training_meta_data[e-1, 1] = gan_loss[0]

    # Discriminator and model accuracy
    training_meta_data[e-1, 2] = d_loss[1]
    training_meta_data[e-1, 3] = gan_loss[1]


    # If at sample interval, print training status and save samples
    if e % sample_interval == 0:
      
        # Print training status
        print("Epoch %d" %e)
        log_mesg = "%d: [Discriminaotr loss: %f, acc: %f]" % (e, d_loss[0], d_loss[1])
        log_mesg = "%s  [GAN loss: %f, acc: %f]" % (log_mesg, gan_loss[0], gan_loss[1])
        print(log_mesg)
        
        # Plot images 
        r, c = 5, 5

        # Create images from the noise (predict the outcome of the noise)
        gen_imgs = generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')
                axs[i,j].axis('off')
                cnt += 1
        plt.show()

# Plot model loss vs epoch
plt.plot(plot_loss_descriminator)
plt.plot(plot_loss_gan)
plt.title('Loss VS Epochs DROPOUT 0.30')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['discriminator loss','GAN Model loss','upperleft'])
plt.show()

# Plot accuracy vs epoch
plt.plot(plot_acc_descriminator)
plt.plot(plot_acc_gan)
plt.title('Accuracy VS Epochs DROPOUT - 0.30')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['discriminator Accuracy','GAN Model Accuracy','upperleft'])
plt.show()

"""**EVERYTHING SAME AS ABOVE - Without Dropout**"""

# The D-dimensional noise vector length
latent_dim = 100

# Optimizer for discriminator, which will have a higher learning rate than adversarial model
def adam_optimizer():
    return adam(lr = 0.2, beta_l = 0.5)

def dis_optimizer():
    #return adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5)

def gan_optimizer():
    #return adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5) 




# Genrerator model
dim = 7
depth = 64+64+64+64
def create_generator():
    generator = Sequential()
    generator.add(Dense(depth, input_dim = latent_dim))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(512))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(1024))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(28*28, activation = 'tanh'))

    #generator.compile(loss = keras.losses.BinaryCrossentropy(),
    #                      optimizer = gan_optimizer(),
    #                      metrics = ['accuracy'])
    return generator

# Discriminator model
def create_discriminator():
    discriminator = Sequential()
    discriminator.add(Dense(1024, input_dim = 28*28))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    ###### discriminator.add(Dropout(0.30)) 

    discriminator.add(Dense(512))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    ###### discriminator.add(Dropout(0.30)) 

    discriminator.add(Dense(256))
    discriminator.add(LeakyReLU(alpha = 0.2))

    discriminator.add(Dense(units = 1, activation = 'sigmoid'))
    discriminator.compile(loss = keras.losses.BinaryCrossentropy(),
                          optimizer = dis_optimizer(),
                          metrics = ['accuracy'])
    return discriminator

# Create adversarial model
def create_gan(discriminator, generator):
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(inputs = gan_input, outputs = gan_output)
    gan.compile(loss = keras.losses.BinaryCrossentropy(),
                optimizer = gan_optimizer(),
                metrics = ['accuracy'])
    return gan

# Creating graph for GAN
generator = create_generator()
discriminator = create_discriminator()
gan = create_gan(discriminator, generator)



# ALGORITHM 1
# Model and training parameters
#ASSIGN VALUES TO THE FOLLOWING VARIABLES
epochs = 100000          # debug using small epochs 5
batch_size = 1024 
sample_interval = 10000      # debug using small interval 1

plot_loss_descriminator = []
plot_acc_descriminator = []
plot_loss_gan = []
plot_acc_gan = []

# Array to save training history
training_meta_data = np.zeros([epochs, 4])

# Training the GAN
for e in range(1, epochs+1):

    # Generate random noise as input
    noise = np.random.normal(0,1, size = (batch_size, latent_dim)) #-0.10

    # Generate fake MNIST images from generated noise
    fake_image = generator.predict(noise)
    #print("fake image shape :", fake_image.shape)

    # Get a random set of real MNIST images
    real_image = data_train[np.random.randint(0, data_train.shape[0], size = batch_size)]
    real_image = real_image.reshape(batch_size, 28*28)
    #print(" Real image shape :", real_image.shape)

    # Concatenate real and fake images into a single array (or batch)
    data_total = np.concatenate([real_image, fake_image])

    # Assign training labels (assign high probability, but not 1, to real images)
    labels_real = np.ones((batch_size))*0.9
    labels_fake = np.zeros((batch_size)) #0.10 
    labels_dis = np.concatenate([labels_real, labels_fake])

    # Allow discriminator parameters to be updated
    discriminator.trainable = True

    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable
    d_loss = discriminator.train_on_batch(data_total, labels_dis)
    plot_loss_descriminator.append(d_loss[0])
    plot_acc_descriminator.append(d_loss[1])


    # Train adversarial model and try to fool discriminator (with incorrect label) 
    # by generating a new batch of noise and assign them labels of real data
    noise = np.random.normal(0,1, size = (batch_size, latent_dim))#-0.10
    labels_gen = np.ones((batch_size))   #*0.9

    # Keep discriminator weights constant while training generator
    discriminator.trainable = False

    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable
    gan_loss = gan.train_on_batch(noise, labels_gen)
    plot_loss_gan.append(gan_loss[0])
    plot_acc_gan.append(gan_loss[1])


    # Save training status
    # Discriminator and model loss
    training_meta_data[e-1, 0] = d_loss[0]
    training_meta_data[e-1, 1] = gan_loss[0]

    # Discriminator and model accuracy
    training_meta_data[e-1, 2] = d_loss[1]
    training_meta_data[e-1, 3] = gan_loss[1]


    # If at sample interval, print training status and save samples
    if e % sample_interval == 0:
      
        # Print training status
        print("Epoch %d" %e)
        log_mesg = "%d: [Discriminaotr loss: %f, acc: %f]" % (e, d_loss[0], d_loss[1])
        log_mesg = "%s  [GAN loss: %f, acc: %f]" % (log_mesg, gan_loss[0], gan_loss[1])
        print(log_mesg)
        
        # Plot images 
        r, c = 5, 5

        # Create images from the noise (predict the outcome of the noise)
        gen_imgs = generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')
                axs[i,j].axis('off')
                cnt += 1
        plt.show()

# Plot model loss vs epoch
plt.plot(plot_loss_descriminator)
plt.plot(plot_loss_gan)
plt.title('Loss VS Epochs NO DROPOUT')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['discriminator loss','GAN Model loss','upperleft'])
plt.show()

# Plot accuracy vs epoch
plt.plot(plot_acc_descriminator)
plt.plot(plot_acc_gan)
plt.title('Accuracy VS Epochs NO DROPOUT')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['discriminator Accuracy','GAN Model Accuracy','upperleft'])
plt.show()

"""[4]. Compare and comment on the results of GAN with dropout and without dropout.

**Without dropouts GAN tends to only produce images of the number 1, with very few other number images on the MNIST data as is seem from the images obtained above.**

**When introduced the dropouts, I observed the images for different values of the dropout rate, and found that the quality and variation of the images produced were good when the dropout rate were close to 0.2 - 0.3. Comparing the case-1 (where I used a dropuout of 0.3) with no dropout, the images with dropout (in case 1) are having a larger variety of numbers which spans a larger range of numbers learned and better quality of images. While with no dropouts the image quality is comparably low and has low variation i.e. mainly outputs the number 1.**

**This shows dropout improves diversity and generalization**

**REDUCING THE DROPOUT RATE FROM 0.30 USED EARLIER TO 0.20, AND KEEPING EVERYTHING SAME**

**USING DROPOUT OF 0.20**
"""

# The D-dimensional noise vector length
latent_dim = 100

# Optimizer for discriminator, which will have a higher learning rate than adversarial model
def adam_optimizer():
    return adam(lr = 0.2, beta_l = 0.5)

def dis_optimizer():
    #return adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5)

def gan_optimizer():
    #return adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)
    return adam(lr = 0.0002, beta_1 = 0.5) 




# Genrerator model
dim = 7
depth = 64+64+64+64
def create_generator():
    generator = Sequential()
    generator.add(Dense(depth, input_dim = latent_dim))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(512))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(1024))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(28*28, activation = 'tanh'))

    #generator.compile(loss = keras.losses.BinaryCrossentropy(),
    #                      optimizer = gan_optimizer(),
    #                      metrics = ['accuracy'])
    return generator

# Discriminator model
def create_discriminator():
    discriminator = Sequential()
    discriminator.add(Dense(1024, input_dim = 28*28))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    discriminator.add(Dropout(0.20))  # Originally 0.30

    discriminator.add(Dense(512))
    discriminator.add(LeakyReLU(alpha = 0.2))   
    
    discriminator.add(Dropout(0.20))   # Originally 0.30

    discriminator.add(Dense(256))
    discriminator.add(LeakyReLU(alpha = 0.2))

    discriminator.add(Dense(units = 1, activation = 'sigmoid'))
    discriminator.compile(loss = keras.losses.BinaryCrossentropy(),
                          optimizer = dis_optimizer(),
                          metrics = ['accuracy'])
    return discriminator

# Create adversarial model
def create_gan(discriminator, generator):
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(inputs = gan_input, outputs = gan_output)
    gan.compile(loss = keras.losses.BinaryCrossentropy(),
                optimizer = gan_optimizer(),
                metrics = ['accuracy'])
    return gan

# Creating graph for GAN
generator = create_generator()
discriminator = create_discriminator()
gan = create_gan(discriminator, generator)



# ALGORITHM 1
# Model and training parameters
#ASSIGN VALUES TO THE FOLLOWING VARIABLES
epochs = 100000          # debug using small epochs 5
batch_size = 1024 
sample_interval = 10000      # debug using small interval 1

plot_loss_descriminator = []
plot_acc_descriminator = []
plot_loss_gan = []
plot_acc_gan = []

# Array to save training history
training_meta_data = np.zeros([epochs, 4])

# Training the GAN
for e in range(1, epochs+1):

    # Generate random noise as input
    noise = np.random.normal(0,1, size = (batch_size, latent_dim)) #-0.10

    # Generate fake MNIST images from generated noise
    fake_image = generator.predict(noise)
    #print("fake image shape :", fake_image.shape)

    # Get a random set of real MNIST images
    real_image = data_train[np.random.randint(0, data_train.shape[0], size = batch_size)]
    real_image = real_image.reshape(batch_size, 28*28)
    #print(" Real image shape :", real_image.shape)

    # Concatenate real and fake images into a single array (or batch)
    data_total = np.concatenate([real_image, fake_image])

    # Assign training labels (assign high probability, but not 1, to real images)
    labels_real = np.ones((batch_size))*0.9
    labels_fake = np.zeros((batch_size)) #0.10 
    labels_dis = np.concatenate([labels_real, labels_fake])

    # Allow discriminator parameters to be updated
    discriminator.trainable = True

    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable
    d_loss = discriminator.train_on_batch(data_total, labels_dis)
    plot_loss_descriminator.append(d_loss[0])
    plot_acc_descriminator.append(d_loss[1])


    # Train adversarial model and try to fool discriminator (with incorrect label) 
    # by generating a new batch of noise and assign them labels of real data
    noise = np.random.normal(0,1, size = (batch_size, latent_dim))#-0.10
    labels_gen = np.ones((batch_size))   #*0.9

    # Keep discriminator weights constant while training generator
    discriminator.trainable = False

    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable
    gan_loss = gan.train_on_batch(noise, labels_gen)
    plot_loss_gan.append(gan_loss[0])
    plot_acc_gan.append(gan_loss[1])


    # Save training status
    # Discriminator and model loss
    training_meta_data[e-1, 0] = d_loss[0]
    training_meta_data[e-1, 1] = gan_loss[0]

    # Discriminator and model accuracy
    training_meta_data[e-1, 2] = d_loss[1]
    training_meta_data[e-1, 3] = gan_loss[1]


    # If at sample interval, print training status and save samples
    if e % sample_interval == 0:
      
        # Print training status
        print("Epoch %d" %e)
        log_mesg = "%d: [Discriminaotr loss: %f, acc: %f]" % (e, d_loss[0], d_loss[1])
        log_mesg = "%s  [GAN loss: %f, acc: %f]" % (log_mesg, gan_loss[0], gan_loss[1])
        print(log_mesg)
        
        # Plot images 
        r, c = 5, 5

        # Create images from the noise (predict the outcome of the noise)
        gen_imgs = generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')
                axs[i,j].axis('off')
                cnt += 1
        plt.show()

# Plot model loss vs epoch
plt.plot(plot_loss_descriminator)
plt.plot(plot_loss_gan)
plt.title('Loss VS Epochs DROPOUT - Now 0.30 reduced to 0.20 ')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['discriminator loss','GAN Model loss','upperleft'])
plt.show()

# Plot accuracy vs epoch
plt.plot(plot_acc_descriminator)
plt.plot(plot_acc_gan)
plt.title('Accuracy VS Epochs DROPOUT - Now 0.30 reduced to 0.20')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['discriminator Accuracy','GAN Model Accuracy','upperleft'])
plt.show()

"""**[5][a]. Comment on importance of hyper-parameter tuning:** Increase or decrease the dropout rate in each dropout layer by 0.1 and train
the network by keeping all the other hyper-parameters the same. Observe
and comment on the results.

**On reducing the dropout value from 0.3 (in Part 1) to 0.20, I observed the following:**

**- The number of entries that are 1 in the case of Dropout of 0.30 is 6, while the number of 1 entries when dropout is 0.2 is 7 (after 100000 iterations). So clearly higher dropout value produces more generalization effect, and captures more number images. As per the quality of the images is concerned the image qualities very close.**

**- Dropout - 0.30 after 100000 iterations: Discriminaotr loss: 0.580228, acc: 0.374023 and GAN loss: 1.581842, acc: 0.066406**

**- Dropout - 0.20 after 100000 iterations: Discriminaotr loss: 0.525017, acc: 0.387207 and GAN loss: 1.820184, acc: 0.056641**

**- So, interms of the Loss and accuracy plots for the dropout values of 0.2 and 0.3, the plots are very similar in the trends. For both the discriminator accuracy values are hovering around 0.5 and gan accuracy is hovering around 0.4 for both the cases.**

**- In terms of image qualities, the final images (after 100000 iterations) and the intermediate images (after 10000, 20000, ...., 90000, 100000 iterations) were slightly clearer & better with dropout value of 0.20 as compared to the dropout value of 0.30**

**BONUS - WITH FIXING DROPOUT OF 0.20, CHANGING OTHER HYPERPARAMETERS TO REPLICATE OR IMPROVE THE ORIGINAL RESULT WHERE DROPOUT OF 0.30 WAS USED**

**NOTE: RAN AND COMPARED THE IMAGES UPTO 60000 iterations AS THE RUNNING TIMES ARE VERY SLOW. RUNNING FOR 60000 Iterations, as the RUNNING WAS VERY SLOW, SINCE THE PROJECT INVOLVED LOT OF GPU USAGE, GOOGLE COLAB AFTER SOME TIME RESTRICTS THE ME FROM USING GPU, SO RUNNING BECAME VERY SLOW. HENCE RUNNING FOR 60000 EPOCHS****

**The Hyperparameter Values set are : withdropout of 0.2, the number of units in the layer are changed, and the Alpha for the LeakyRELU is reduced from 0.2 to 0.1. Since in the previous part, I chose to reduce the DROPOUT value from 0.3 to 0.2, so the model will be doing a little bad on the generalization (variation of the images obtained), but with these parameter value changes, the images quality is similar or slightly improved, although the generalization is lower as compared to the case with 0.3 dropout value.**
"""

# The D-dimensional noise vector length
latent_dim = 100

# Optimizer for discriminator, which will have a higher learning rate than adversarial model

def dis_optimizer():
    return adam(lr = 0.0002, beta_1 = 0.5)

def gan_optimizer():
    return adam(lr = 0.0002, beta_1 = 0.5) 




# Genrerator model
dim = 7
def create_generator():
    generator = Sequential()
    generator.add(Dense(300, input_dim = latent_dim))
    generator.add(LeakyReLU(alpha = 0.1))

    generator.add(Dense(600))
    generator.add(LeakyReLU(alpha = 0.1))

    generator.add(Dense(1200))
    generator.add(LeakyReLU(alpha = 0.1))

    generator.add(Dense(28*28, activation = 'tanh'))


    return generator

# Discriminator model
def create_discriminator():
    discriminator = Sequential()
    discriminator.add(Dense(1024, input_dim = 28*28))
    discriminator.add(LeakyReLU(alpha = 0.1))
    
    discriminator.add(Dropout(0.20))  # Originally 0.30

    discriminator.add(Dense(512))
    discriminator.add(LeakyReLU(alpha = 0.1))   
    
    discriminator.add(Dropout(0.20))   # Originally 0.30

    discriminator.add(Dense(256))
    discriminator.add(LeakyReLU(alpha = 0.1))

    discriminator.add(Dense(units = 1, activation = 'sigmoid'))
    discriminator.compile(loss = keras.losses.BinaryCrossentropy(),
                          optimizer = dis_optimizer(),
                          metrics = ['accuracy'])
    return discriminator

# Create adversarial model
def create_gan(discriminator, generator):
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(inputs = gan_input, outputs = gan_output)
    gan.compile(loss = keras.losses.BinaryCrossentropy(),
                optimizer = gan_optimizer(),
                metrics = ['accuracy'])
    return gan

# Creating graph for GAN
generator = create_generator()
discriminator = create_discriminator()
gan = create_gan(discriminator, generator)



# ALGORITHM 1
# Model and training parameters
#ASSIGN VALUES TO THE FOLLOWING VARIABLES
epochs = 100000        # debug using small epochs 5
batch_size = 1024 
sample_interval = 10000      # debug using small interval 1

plot_loss_descriminator = []
plot_acc_descriminator = []
plot_loss_gan = []
plot_acc_gan = []

# Array to save training history
training_meta_data = np.zeros([epochs, 4])

# Training the GAN
for e in range(1, epochs+1):

    # Generate random noise as input
    noise = np.random.normal(0,1, size = (batch_size, latent_dim)) #-0.10

    # Generate fake MNIST images from generated noise
    fake_image = generator.predict(noise)
    #print("fake image shape :", fake_image.shape)

    # Get a random set of real MNIST images
    real_image = data_train[np.random.randint(0, data_train.shape[0], size = batch_size)]
    real_image = real_image.reshape(batch_size, 28*28)
    #print(" Real image shape :", real_image.shape)

    # Concatenate real and fake images into a single array (or batch)
    data_total = np.concatenate([real_image, fake_image])

    # Assign training labels (assign high probability, but not 1, to real images)
    labels_real = np.ones((batch_size))*0.9
    labels_fake = np.zeros((batch_size)) #0.10 
    labels_dis = np.concatenate([labels_real, labels_fake])

    # Allow discriminator parameters to be updated
    discriminator.trainable = True

    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable
    d_loss = discriminator.train_on_batch(data_total, labels_dis)
    plot_loss_descriminator.append(d_loss[0])
    plot_acc_descriminator.append(d_loss[1])


    # Train adversarial model and try to fool discriminator (with incorrect label) 
    # by generating a new batch of noise and assign them labels of real data
    noise = np.random.normal(0,1, size = (batch_size, latent_dim))#-0.10
    labels_gen = np.ones((batch_size))   #*0.9

    # Keep discriminator weights constant while training generator
    discriminator.trainable = False

    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable
    gan_loss = gan.train_on_batch(noise, labels_gen)
    plot_loss_gan.append(gan_loss[0])
    plot_acc_gan.append(gan_loss[1])


    # Save training status
    # Discriminator and model loss
    training_meta_data[e-1, 0] = d_loss[0]
    training_meta_data[e-1, 1] = gan_loss[0]

    # Discriminator and model accuracy
    training_meta_data[e-1, 2] = d_loss[1]
    training_meta_data[e-1, 3] = gan_loss[1]


    # If at sample interval, print training status and save samples
    if e % sample_interval == 0:
      
        # Print training status
        print("Epoch %d" %e)
        log_mesg = "%d: [Discriminaotr loss: %f, acc: %f]" % (e, d_loss[0], d_loss[1])
        log_mesg = "%s  [GAN loss: %f, acc: %f]" % (log_mesg, gan_loss[0], gan_loss[1])
        print(log_mesg)
        
        # Plot images 
        r, c = 5, 5

        # Create images from the noise (predict the outcome of the noise)
        gen_imgs = generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')
                axs[i,j].axis('off')
                cnt += 1
        plt.show()

"""[6]. Answer the following questions:

1.   Why does the accuracy of the discriminator remain around 50%? Is this a good trait of the GAN? 

  ANS: **This trait of the discriminator, i.e. having accuracy around 0.5 is very good and is a GOOD trait of the GAN. The reason for this accuracy to be around 0.5 is as follows:**

**- Ideally we want the generator to be able to fool Discriminator every time into thinking that a fake example is a real one and vice versa. It wants that the discriminator shouldn't be able to differentiate between real and fake examples.**

**- We don't show the discriminator real examples exclusively for it to tell if it's always a real one. Ideally we show the discriminator real examples 50% of the time and fake images for the other 50% of the time.**

**- For a good performing discriminator we want it to think that it's a real example when it really is and it's a fake one when it is really a fake one. But, when the fakes are really good, it won't be able to tell a difference.**

**- It will obviously do some miss-classifications, as on average, the real and fake ones have occuring probability of 50% each, if they have the same number of samples. If everything's perfect, the best Discriminator can do is, to guess whether it's a fake or a real example, with equal probability of 50%.**

**- Moreover this can not be increased beyond 50%. Say, if the discriminator always outputs '1', for real examples and always outputs '0', for fake examples, this implies that our generator is really bad, or the discriminator is broken.**


**So 50% is the best Discriminator can do and is a good trait for GAN**


2.   How could this model be modified to produce cleaner (less noisy) images? 

  ANS: **Doing data augmentation can help in better GAN learning and thus produce cleaner images**

  **Doing Batch Normalization**

  **Using a Guassian Latent Space**

  **Using Noisy Labels - for example create 1000 samples of real labels and flip then with a p% probability, then doing the same thing for fake labels**

#Part 2: Generating samples using trained generator

**SHOWING 10 images using the trained generator from PART 1 (1)**

**NOTE : RUNNING FOR 50000 Iterations, as the RUNNING WAS VERY SLOW, SINCE THE PROJECT INVOLVED LOT OF GPU USAGE, GOOGLE COLAB AFTER SOME TIME RESTRICTS THE ME FROM USING GPU, SO RUNNING BECAME VERY SLOW. HENCE RUNNING FOR 50000 EPOCHS**
"""

#Import necessary packages
import numpy as np
import keras
from keras.layers import Dense, Dropout, Input
from keras.models import Model,Sequential
from keras.datasets import mnist
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam
from keras.models import load_model
import matplotlib.pyplot as plt

#Load MNIST data and normalize to [-1, 1]

#Load MNIST data and normalize to [0,1]
(data_train, _), (data_test, _) = mnist.load_data()
data_train = data_train/255.0
data_test = data_test/255.0
# Shifting the above data in [-0.5, 0.5] by substracting 0.5 from above
data_train = data_train - 0.5
data_test = data_test - 0.5
#  Now multiplying the above by 2 to scale it to [-1,1]
data_train = data_train * 2
data_test = data_test * 2
print("shape training", np.shape(data_train))
print("shape testing", np.shape(data_test))

# The D-dimensional noise vector length
latent_dim = 100

# Optimizer for discriminator, which will have a higher learning rate than adversarial model

def dis_optimizer():
    #return adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)
    return Adam(lr = 0.0002, beta_1 = 0.5)

def gan_optimizer():
    #return adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)
    return Adam(lr = 0.0002, beta_1 = 0.5) 




# Genrerator model
dim = 7
depth = 64+64+64+64 #256
def create_generator():
    generator = Sequential()
    generator.add(Dense(depth, input_dim = latent_dim))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(512))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(1024))
    generator.add(LeakyReLU(alpha = 0.2))

    generator.add(Dense(28*28, activation = 'tanh'))

    #generator.compile(loss = keras.losses.BinaryCrossentropy(),
    #                      optimizer = gan_optimizer(),
    #                      metrics = ['accuracy'])
    return generator

# Discriminator model
def create_discriminator():
    discriminator = Sequential()
    discriminator.add(Dense(1024, input_dim = 28*28))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    discriminator.add(Dropout(0.30)) #0.25

    discriminator.add(Dense(512))
    discriminator.add(LeakyReLU(alpha = 0.2))
    
    discriminator.add(Dropout(0.30)) #0.25

    discriminator.add(Dense(256))
    discriminator.add(LeakyReLU(alpha = 0.2))

    discriminator.add(Dense(units = 1, activation = 'sigmoid'))
    discriminator.compile(loss = keras.losses.BinaryCrossentropy(),
                          optimizer = dis_optimizer(),
                          metrics = ['accuracy'])
    return discriminator

# Create adversarial model
def create_gan(discriminator, generator):
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    x = generator(gan_input)
    gan_output = discriminator(x)
    gan = Model(inputs = gan_input, outputs = gan_output)
    gan.compile(loss = keras.losses.BinaryCrossentropy(),
                optimizer = gan_optimizer(),
                metrics = ['accuracy'])
    return gan

# Creating graph for GAN
generator = create_generator()
discriminator = create_discriminator()
gan = create_gan(discriminator, generator)



# ALGORITHM 1
# Model and training parameters
#ASSIGN VALUES TO THE FOLLOWING VARIABLES
epochs = 50000          # debug using small epochs 5
batch_size = 1024 
sample_interval = 10000      # debug using small interval 1

plot_loss_descriminator = []
plot_acc_descriminator = []
plot_loss_gan = []
plot_acc_gan = []

# Array to save training history
training_meta_data = np.zeros([epochs, 4])

# Training the GAN
for e in range(1, epochs+1):

    # Generate random noise as input
    noise = np.random.normal(0,1, size = (batch_size, latent_dim)) #-0.10

    # Generate fake MNIST images from generated noise
    fake_image = generator.predict(noise)
    #print("fake image shape :", fake_image.shape)

    # Get a random set of real MNIST images
    real_image = data_train[np.random.randint(0, data_train.shape[0], size = batch_size)]
    real_image = real_image.reshape(batch_size, 28*28)
    #print(" Real image shape :", real_image.shape)

    # Concatenate real and fake images into a single array (or batch)
    data_total = np.concatenate([real_image, fake_image])

    # Assign training labels (assign high probability, but not 1, to real images)
    labels_real = np.ones((batch_size))*0.9
    labels_fake = np.zeros((batch_size)) #0.10 
    labels_dis = np.concatenate([labels_real, labels_fake])

    # Allow discriminator parameters to be updated
    discriminator.trainable = True

    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable
    d_loss = discriminator.train_on_batch(data_total, labels_dis)
    plot_loss_descriminator.append(d_loss[0])
    plot_acc_descriminator.append(d_loss[1])


    # Train adversarial model and try to fool discriminator (with incorrect label) 
    # by generating a new batch of noise and assign them labels of real data
    noise = np.random.normal(0,1, size = (batch_size, latent_dim))#-0.10
    labels_gen = np.ones((batch_size))   #*0.9

    # Keep discriminator weights constant while training generator
    discriminator.trainable = False

    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable
    gan_loss = gan.train_on_batch(noise, labels_gen)
    plot_loss_gan.append(gan_loss[0])
    plot_acc_gan.append(gan_loss[1])


    # Save training status
    # Discriminator and model loss
    training_meta_data[e-1, 0] = d_loss[0]
    training_meta_data[e-1, 1] = gan_loss[0]

    # Discriminator and model accuracy
    training_meta_data[e-1, 2] = d_loss[1]
    training_meta_data[e-1, 3] = gan_loss[1]


    # If at sample interval, print training status and save samples
    if e % sample_interval == 0:
      
        # Print training status
        print("Epoch %d" %e)
        log_mesg = "%d: [Discriminaotr loss: %f, acc: %f]" % (e, d_loss[0], d_loss[1])
        log_mesg = "%s  [GAN loss: %f, acc: %f]" % (log_mesg, gan_loss[0], gan_loss[1])
        print(log_mesg)
        
        # Plot images 
        r, c = 5, 5

        # Create images from the noise (predict the outcome of the noise)
        gen_imgs = generator.predict(noise)

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')
                axs[i,j].axis('off')
                cnt += 1
        plt.show()

# Generate ten images from Gaussian noise using the trained generator from Part 1
noise = np.random.normal(0,1,[10,100])
generated_images = generator.predict(noise)

# Re-scale generated images to lie in [0, 1]
generated_images = 0.5*(generated_images + 1.0)

# Visualize generated noise
r, c = 2, 5
fig, axs = plt.subplots(r, c)
cnt = 0
for i in range(r):
    for j in range(c):
        axs[i,j].imshow((noise[cnt].reshape(10, 10)), cmap='gray')
        axs[i,j].axis('off')
        cnt += 1
plt.show()

# Visualize generated Samples
r, c = 2, 5
fig, axs = plt.subplots(r, c)
cnt = 0
for i in range(r):
    for j in range(c):
        axs[i,j].imshow((generated_images[cnt].reshape(28, 28)), cmap='gray')
        axs[i,j].axis('off')
        cnt += 1
plt.show()

"""**Using The Generator in Part 1, (1)**

#Part 3: Testing accuracy of generated images on ten samples
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive', force_remount = True)
# %cd /gdrive/'My Drive'/
mnist_classifier = load_model('mnist_classifier.h5')

from keras.utils import np_utils

# ASSIGN CLASSES
labels = [9,2,7,1,6,8,2,1,7,1]

# Convert integer labels to one-hot labels 
labels = keras.utils.np_utils.to_categorical(labels, num_classes=10)

# Show classifications
predicted_labels = mnist_classifier.predict(generated_images)

print("Prediction for pre-trained classifier")
print(np.argmax(predicted_labels, axis = 1))

# Evaluate accuracy
accuracy = mnist_classifier.evaluate(generated_images, labels)
print("Accuracy : %.2f%%" %(accuracy[1]*100))

"""State the accuracy of the classifier on your ten generated images. Based on
this accuracy, would you say your generator does well in producing images
comparable to those in the MNIST dataset of hand-written digits? Why or
why not?

**The accuracy of the classifier is 90% over the generated images. Yes the generator does well in producing images comparable to those in the MNIST dataset of hand-written digits. It is important to note that the labels I entered here were obtained after 60000 epochs, so if I ran it for 100000 epochs then entered labels would have been more clearer and it would have been easier to put the correct labels. But even with that achieving an accuracy of 90% is really good.**

**Since the given classifier is trained on the MNIST data, and its accuracy on the generated data is high this menas that the generated images are very similar to the MNIST data.**

**The pre-trained classifier matched the naked eye observation over 90% of the images so it is good.**

In this project, we only tested the performance of the pre-trained classifier
on ten samples and used its result to determine the robustness of the generator.
How could we better assess the quality of the generated images using
this pre-trained classifier and the saved generator?

**We can introduce other random noises, and then see the quality of the generated images. When calculating accuracy, we should also have a larger pool of images, having just 10 images can be misleading the actual accuracy. We should also do this with respect to the number of epochs, because for lesser number of epochs, the resultant images are not clear and the person putting in the corresponding labels may actually be giving incorrect labels, which would eventually result in low accuracy. Thus the number of epochs should be large enough to allow the person to feed in correct labels.**
"""