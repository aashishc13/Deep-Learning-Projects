# -*- coding: utf-8 -*-
"""Project_4_ASHISH_CHANDRA_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rjh_iL3wQDtrahOY13gsHCQIqJR39bMp

#Student Name: ASHISH CHANDRA
#ECE 595 Machine Learning II
#Project 4: Adversarial Machine Learning - Student Code
"""

#Install Cleverhans (version Cleverhans 2.1.0 is most compatable with Python 2.x)
!pip install cleverhans==2.1.0

#Import necessary packages
from keras.datasets import mnist
from keras import Sequential
from keras.layers import Dense, BatchNormalization
from keras import backend
import keras
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from cleverhans.utils_keras import KerasModelWrapper
from cleverhans.attacks import FastGradientMethod, MadryEtAl, DeepFool, CarliniWagnerL2

"""#Part 1: Training a target classifier

**Defining the data set**
"""

# Load data MNIST data and normalize to [0, 1]
(data_train, labels_train), (data_test, labels_test) = mnist.load_data()
data_train = data_train/255.0
data_test = data_test/255.0
print("shape training", np.shape(data_train))
print("shape testing", np.shape(data_test))

#Reshape training and testing data into 784-dimensional vectors 
data_train = data_train.reshape(60000,28*28)
data_test = data_test.reshape(10000, 28*28)
print("shape training after reshaping", np.shape(data_train))
print("shape testing after reshaping", np.shape(data_test))

#Convert integer labels for training and testing data into one-hot vectors 
labels_train = keras.utils.np_utils.to_categorical(labels_train, num_classes=10)
labels_test = keras.utils.np_utils.to_categorical(labels_test, num_classes=10)

#Create classifier architecture, compile it, and train it
def create_classifier():
  classifier = Sequential()
  # Hidden layer 1
  classifier.add(Dense(100,
                    activation = "relu",
                    use_bias = True,
                    kernel_initializer = "uniform",
                    input_dim = 784))

  # Batch normalization
  classifier.add(BatchNormalization())

  # Hidden layer 2
  classifier.add(Dense(100,
                    activation = "relu",
                    use_bias = True,
                    kernel_initializer = "uniform"))
  
  # Batch normalization
  classifier.add(BatchNormalization())

  # out put layer
  classifier.add(Dense(10,
                    activation = "softmax",
                    kernel_initializer = "uniform"))
  
  return classifier

create_classifier = create_classifier()
create_classifier.summary()

# compile model using an appropriate loss and optimization algorithm
create_classifier.compile(loss = keras.losses.CategoricalCrossentropy(),
                    optimizer = 'adam',
                    metrics = ['accuracy'])

# Training the data
create_classifier_mdata = create_classifier.fit(data_train, labels_train,
                                                  validation_data = (data_test, labels_test),
                                                  epochs = 50,
                                                  batch_size = 256,
                                                  shuffle = True)

#Plot loss vs epoch
plt.plot(create_classifier_mdata.history['loss'])
plt.plot(create_classifier_mdata.history['val_loss'])
plt.title('Classifier Loss VS Epochs')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

#Plot accuracy vs epoch
plt.plot(create_classifier_mdata.history['accuracy'])
plt.plot(create_classifier_mdata.history['val_accuracy'])
plt.title('Classifier accuracy vs Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

#Print accuracy of classifier on MNIST testing data
scores = create_classifier.evaluate(data_test, labels_test)
print("Accuracy : %.2f%%" %(scores[1]*100))

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
!pip install cleverhans==2.1.0
import tensorflow
print(tensorflow.__version__)

# Edit the classifier name fed into KerasModel Wrapper with the name of the 

#Get TensorFlow Session to pass into Cleverhans modules
sess = backend.get_session()
#sess = tf.compat.v1.Session()

#Create wrapper for classifier model so that it can be passed into Cleverhans modules
wrap = KerasModelWrapper(create_classifier)

"""#Part 2: The Fast Gradient Method (FGM)"""

#Implementing the FGSM attack

#FGM Instance on trained classifier from Part 1
fgsm = FastGradientMethod(wrap, sess = sess)

#Attack parameters
fgsm_params = {'eps': 0.25,
               'clip_min': 0.0,
               'clip_max': 1.0}

#Generate adversarial data
fgsm_attack_data = fgsm.generate_np(data_test, **fgsm_params)

#Evaluate accuracy on target classifier 
fgsm_adv_scores = create_classifier.evaluate(fgsm_attack_data, labels_test)

print("FGSM adv attack accuracy : %.2f%%" %(fgsm_adv_scores[1]*100))

"""**FGSM adv attack accuracy : 7.62%**

**Showing images from the testing set befoe adding adv perturbation**
"""

#Show ten original samples and their corresponding adversarial samples
# Showing first 10 original samples in the TEST SET
n = 10
print(" First 10 samples of the Testing set")
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(data_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

"""**Images of fgsm attack data**"""

print(" First 10 fgsm data")
# FGSM ATTACK DATA IMAGES

plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(fgsm_attack_data[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

"""**Showing images from the testing set after adding adv perturbation**

"""

print(" First 10 samples of the Testing set with additional adversarial purturbation")
# Showing first 10 images of the TEST SET after adding the Adv purturbation
#print(np.shape(data_test))
#print(np.shape(fgsm_attack_data))

# TEST DATA + FGSM ATTACK DATA IMAGES
Images_after_adv_perturb = data_test + fgsm_attack_data
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(Images_after_adv_perturb[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

#Implementing Detection via Autoencoders
def autoencoder():
    ae = Sequential()
    ae.add(Dense(400, activation=None, kernel_initializer="normal", input_dim=784))
    ae.add(Dense(200, activation=None, kernel_initializer='normal'))
    ae.add(Dense(100, activation=None, kernel_initializer='normal'))
    ae.add(Dense(200, activation=None, kernel_initializer='normal'))
    ae.add(Dense(400, activation=None, kernel_initializer='normal'))
    ae.add(Dense(784, activation='sigmoid', kernel_initializer='normal'))
    return ae

autoencoder = autoencoder()
autoencoder.summary()

#Create and train the autoencoder using the mean squared error loss and adam optimizer

autoencoder.compile(loss = keras.losses.MeanSquaredError(),
                    optimizer = 'adam',
                    metrics = ['accuracy'])

print(np.shape(data_train))
print(np.shape(data_test))

print(np.shape(labels_train))
print(np.shape(labels_test))
autoencoder_history = autoencoder.fit(data_train, data_train,
                                    validation_data = (data_test, data_test),
                                    epochs = 50,
                                    batch_size = 256,
                                    shuffle = True)

#Using the autoencoder for detection and to determine a threshold 

# Create adversarial examples using FGSM on training data
fgsm_attack_train_data = fgsm.generate_np(data_train, **fgsm_params)

# Obtain reconstruction errors on training set and determine a threshold 
reconstructions = autoencoder.predict(fgsm_attack_train_data)

error = keras.losses.mean_squared_error(reconstructions, fgsm_attack_train_data)

# Convert error tensor into NumPy array 
# CHANGE NAME 'error' TO WHAT YOU CALLED IT ABOVE
error = error.eval(session=sess)

# Determine threshold (based on min in this case) and print it 
threshold = np.amin(error)
print("threshold : min error value =", threshold)



# TESTING DATA -> Adversory -> Autoencoder
# Calculate error of adversarial testing set
reconstructions_test = autoencoder.predict(fgsm_attack_data)

error_test_set = keras.losses.mean_squared_error(reconstructions_test, fgsm_attack_data)
error_test_set = error_test_set.eval(session = sess)


# Determine how many examples are above threshold and consider them adversarial true positive count
# No. of samples out of 10000 testing samples, who's error exceeds the threshold error value ARE TRUE POSITIVE
num_true_positive = 0
for i in error_test_set:
  if (i > threshold):
    num_true_positive = num_true_positive + 1

# Print number of true positive samples
print("Number of True Positive samples in the testing data :", num_true_positive)




# Determine false positives on benign testing set
# TESTING DATA -> AUTOENCODER
reconstruction_test_false_positve = autoencoder.predict(data_test)
error_test_set_no_adversory = keras.losses.mean_squared_error(reconstruction_test_false_positve, data_test)
error_test_set_no_adversory = error_test_set_no_adversory.eval(session = sess)


# Count false positives = num of errors in error_test_set_no_adversory > threshold
num_false_positive = 0
for i in error_test_set_no_adversory:
  if (i > threshold):
    num_false_positive = num_false_positive + 1

# Determine how many examples are above threshold and consider them adversarial (false positive count)
# Print number of false positive samples
print("Number of False Positive samples in the testing data :", num_false_positive)

"""**'threshold : min error value =', 0.020650662**

**Number of True Positive samples in the testing data :', 10000**

**Number of False Positive samples in the testing data :', 0**

#Part 3: Projected Gradient Descent
"""

#Implementing the PGD attack

#PGD Instance on trained classifier from Part 1
pgd = MadryEtAl(wrap, sess=sess)

#Attack parameters
pgd_params = {'eps': 0.25,
              'eps_iter': 0.01,
              'nb_iter': 20,
              'clip_min': 0.0,
              'clip_max': 1.0}

#Generate adversarial data
pgd_attack_data = pgd.generate_np(data_test, **pgd_params)

#Evaluate accuracy of perturbed data on target classifier
pgd_adv_scores = create_classifier.evaluate(pgd_attack_data, labels_test)

print("PGD adv attack accuracy : %.2f%%" %(pgd_adv_scores[1]*100))

#Show ten original samples and their corresponding adversarial samples
# Showing first 10 original samples in the TEST SET
n = 10
print(" First 10 samples of the Testing set")
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(data_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

"""**Ploting only the pgd_attack-data**"""

print(" ploting only the pgd attack data")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(pgd_attack_data[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

print(" First 10 samples of the Testing set with additional adversarial purturbation")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

Images_after_adv_perturb = data_test + pgd_attack_data
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(Images_after_adv_perturb[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

#Implementing the adversarial training defense
adv_trained_clf = create_classifier
defense_data = pgd.generate_np(data_train, **pgd_params)
defense_data = np.concatenate((defense_data, data_train))
adv_trained_clf.compile(loss='categorical_crossentropy', 
                        optimizer='Adam', 
                        metrics=['accuracy'])

adv_trained_clf_mdata = adv_trained_clf.fit(defense_data, 
                                            np.concatenate((labels_train,labels_train)), 
                                            epochs=50, 
                                            batch_size=256, 
                                            shuffle=True)

#Using the defense to evaluate the accuracy of the perturbed data
#FILL THIS IN
pgd_adv_scores_new = adv_trained_clf.evaluate(pgd_attack_data, labels_test)
print("PGD Adversarial Attack Accuracy on retrained model: %.2f%% " %(pgd_adv_scores_new[1]*100))

"""#Part 4: Carlini and Wagner Attack"""

#Implementing the CW attack

#CW Instance on trained classifier from Part 1
CW = CarliniWagnerL2(wrap, sess = sess)


#Attack parameters
CW_params = {'binary_search_steps' : 1,
            'y' : None,
            'learning_rate' : 1.25,
            'batch_size' : 16,
            'initial_const' : 10,
            'clip_min' : 0.0,
            'clip_max' : 1.0,
            'max_iterations' : 50
            }


#Generate adversarial data
CW_attack_data = CW.generate_np(data_test, **CW_params)


#Evaluate accuracy of perturbed data on target classifier
CW_adv_scores = create_classifier.evaluate(CW_attack_data, labels_test)

print("CW adv attack accuracy : %.2f%%" %(CW_adv_scores[1]*100))

"""**CW adv attack accuracy : 1.42%**"""

#Show ten original samples and their corresponding adversarial samples
# Showing first 10 original samples in the TEST SET
n = 10
print(" First 10 samples of the Testing set")
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(data_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

"""**Plotting only the  CW_attack_data**"""

print(" First 10 samples of the Testing set with additional adversarial purturbation")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(CW_attack_data[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

print(" First 10 samples of the Testing set with additional adversarial purturbation")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

Images_after_adv_perturb = data_test + CW_attack_data
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(Images_after_adv_perturb[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

#Implementing the dimensionality reduction (PCA) defense 

#Calculate PCA projection
pca = PCA(100)
pca.fit(data_train)
pca_train = pca.transform(data_train)
pca_test = pca.transform(data_test)

#Transform perturbed CW data using the subspace from the original training data

# create model for PCA
def pca_model():
    model = Sequential()
    
    # first hidden layer
    model.add(Dense(100,
                    activation = "relu",
                    use_bias = True,
                    kernel_initializer = "uniform",
                    input_dim = 100))
    
    # Batch normalization
    model.add(BatchNormalization()) 

    # second hidden layer
    model.add(Dense(100,
                    activation = "relu",
                    use_bias = True,
                    kernel_initializer = "uniform"))

    # Batch normalization
    model.add(BatchNormalization()) 
  
    # output layer
    model.add(Dense(10,
                    activation = "softmax",
                    kernel_initializer = "uniform"))

    return model


pca_model = pca_model()
pca_model.summary()

# compile model using an categorical cross entropy loss and adam optimization
pca_model.compile(loss = keras.losses.CategoricalCrossentropy(),
                    optimizer = 'adam',
                    metrics = ['accuracy'])

# Training the data
pca_model_mdata = pca_model.fit(pca_train, labels_train,
                                                  validation_data = (pca_test, labels_test),
                                                  epochs = 50,
                                                  batch_size = 256,
                                                  shuffle = True)

# Commented out IPython magic to ensure Python compatibility.
#Using the defense (and comparing to baseline accuracy)

pca_attack_data = pca.transform(CW_attack_data)
CW_adv_scores_new = pca_model.evaluate(pca_attack_data, labels_test)

print("CW Adversarial Accuracy on testing set using dimensionality reduction: %.2f%% " 
#       %(CW_adv_scores_new[1]*100))

"""#Part 5: DeepFool """

#Implementing the DeepFool attack

#DeepFool Instance on trained classifier from Part 1
DF = DeepFool(wrap, sess = sess)

#Attack parameters
DF_params = {'nb_candidate' : 10,
             'max_iter' : 50,
             'clip_min' : 0.0,
             'clip_max' : 1.0,
             'eps' : 0.25  
            }

#Generate adversarial data
DF_attack_data = DF.generate_np(data_test, **DF_params)


#Evaluate accuracy of perturbed data on target classifier
DF_adv_scores = create_classifier.evaluate(DF_attack_data, labels_test)

print("DF adv attack accuracy : %.2f%%" %(DF_adv_scores[1]*100))

#Show ten original samples and their corresponding adversarial samples
# Showing first 10 original samples in the TEST SET
n = 10
print(" First 10 samples of the Testing set")
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(data_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

"""**Plotting only the DF_attack_data images**"""

print(" First 10 samples of the Testing set with only DF_attack_data")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(DF_attack_data[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

print(" First 10 samples of the Testing set with additional adversarial purturbation")
# Showing first 10 images of the TEST SET after adding the Adv purturbation

Images_after_adv_perturb = data_test + DF_attack_data
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(Images_after_adv_perturb[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

#Implementing the Denoising Autoencoder Defense

def autoencoder():
    ae = Sequential()
    ae.add(Dense(400, activation=None, kernel_initializer="normal", input_dim=784))
    ae.add(Dense(200, activation=None, kernel_initializer='normal'))
    ae.add(Dense(100, activation=None, kernel_initializer='normal'))
    ae.add(Dense(200, activation=None, kernel_initializer='normal'))
    ae.add(Dense(400, activation=None, kernel_initializer='normal'))
    ae.add(Dense(784, activation='sigmoid', kernel_initializer='normal'))
    return ae

#Create training data for DAE 
df_attack_data_train = DF.generate_np(data_train, **DF_params)
data_total_train = np.concatenate([df_attack_data_train, data_train])

#Create and train DAE graph
dae = autoencoder()
dae.compile(loss = 'mean_squared_error', 
            optimizer='adam')

dae.fit(data_total_train, data_total_train, 
        validation_data = (data_test, data_test), 
        epochs = 50, 
        batch_size = 128, 
        shuffle = True)

# Commented out IPython magic to ensure Python compatibility.
#Use DAE to to remove adversarial perturbation 
reconstructions_dae = dae.predict(DF_attack_data)


#Evaluate accuracy of DF samples after denoising  #Doing with DF instead of FGM
DF_adv_scores_dae = create_classifier.evaluate(reconstructions_dae, labels_test)

print("DF Adversarial Accuracy after forward propagating through DAE: %.2f%% "
#          %(DF_adv_scores_dae[1]*100))

#Show ten samples of adversarial samples after denoising
n = 10
plt.figure(figsize=(20,4))
for i in range(n):
  ax = plt.subplot(1,n,i+1)
  plt.imshow(reconstructions_dae[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)